\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 14, 2022}
\author{Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
		\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
		\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
		\item This problem set is due before class on Monday February 14, 2022. No late assignments will be accepted.
		\item Total available points for this homework is 80.
	\end{itemize}

	
	%	\vspace{.25cm}
	
%\noindent In this problem set, you will run several regressions and create an add variable plot (see the lecture slides) in \texttt{R} using the \texttt{incumbents\_subset.csv} dataset. Include all of your code.

	\vspace{.25cm}
\section*{Question 1} %(20 points)}
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq x) \frac{\sqrt {2\pi}}{x} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8x^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).
	
\vspace{5cm}
Write an R function that implements this test where the reference distribution is normal.

Set the seed
	- set.seed(123)

Problem Set Broken down into 4 parts: 
\vspace{1cm}

1) DATA GENERATION, 2) CALCULATE THEORETICAL AND EMPIRICAL CDFS, 3) CALCULATE D-STAT, 4) CALCULATE P-VALUE AND DRAW CONCLUSIONS 
\vspace{1cm}

1) DATA GENERATION

Create 1000 Cauchy random variables and 1000 normal variables using rcauchy and rnorm functions
	
	- x <- (rcauchy(1000, location = 0, scale = 1))

	- x

	- y <- rnorm(1000)

	- y

Make dataframe from x and y

	- data <- data.frame(x, y)
\vspace{1cm}

2) FIND EMPIRICAL and THEORETICAL CDFs

Create empirical distribution of observed data using ecdf() function
	
	-  ECDF <- ecdf(data(dollar))x) 

	- ECDF

	- empiricalCDF <- ECDF(data(dollar)x)

	- empiricalCDF

Create theoretical data with normal distribution using ecdf() function 

	- TCDF <- ecdf(data(dollar)y)
	- TCDF
	- theoreticalCDF <- TCDF(data(dollar)y)
	- theoreticalCDF

ALT method: Can also use pnorm(data, mean, sd) instead of ecdf for reference distribution

	- mean(data(dollar)y) ANS = [1] 0.03609765
	
	- sd(data(dollar)y) ANS = [1] 1.003827

	- TCDF2 <- pnorm(data(dollar)y, 0.03609765, 1.003827)

	- TCDF2

	- theoreticalCDF2 <- TCDF(data(dollar)y)

	- theoreticalCDF2 
	
	ANS = Same results as using ecdf() function 
\vspace{1cm}

3) CALCULATE T-STAT NAMED D

	- D <- max(abs(empiricalCDF - pnorm(theoreticalCDF)))
	
	- D 
 ANS = [1] 0.8344237
\vspace{1cm}

4) CALCULATE P-VALUE + DRAW CONCLUSIONS

Install package 'dgof' to use ks.test function

	- install.packages('dgof')

	- library('dgof')

Try using ks.test function on theoreticalCDF with normal distribution

	- ks.test(empiricalCDF, theoreticalCDF)
	
ANS = P-value = 1

Try using ks.test of empricalCDFs normality using 'pnorm' as second argument
	
	- ks.test(empiricalCDF, 'pnorm') 
	
	ANS = D = 0.5004, p-value < 2.2e-16

P value far greater than 0.5, therefore indicating that ecdf is far from normal. 
\vspace{1cm}

\section*{Question 2}% (20 points)}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}

First set the seed 

	- set.seed (123)
	\vspace{.5cm}

Create dataframe with x and y variables

	- data <- data.frame(x = runif(200, 1, 10)) 
	data(dollar)y <- 0 + 2.75*data(dollar)x + rnorm(200, 0, 1.5)
\vspace{.5cm}

PART ONE: Try OLS method using lm() function

	- lm <- lm(data(dollar)y ~ data(dollar)x)
	
	- lm
	
ANS: [1] Coefficients: Intercept = -05055  Slope = 2.8166
 
\vspace{.5cm}
PART TWO: Try alternate 'BFGS' method. ANS should be equivalent to lm()

Implement original function minrss to minimise the residual sum of squares

	- minrss <- function(data, par) { 
	with(data, sum((par[1] + par[2] * x - y)[expoent]2))
	}

Using optim() function following quasi-Newton BFGS method 
	
\vspace{.2cm}
	- ?optim
	
	- BFGSresult <- optim(par = c(0, 1), fn = min.RSS, data = data, gr=NULL, method = 'BFGS', lower = -Inf, upper = Inf, control = list(), hessian = FALSE)
	
	- BFGSresult
\vspace{.2cm}

ANS = [1] -0.5055275  2.8165749

Coefficients using BFGS method equivalent to lm() function output. 



\end{document}
